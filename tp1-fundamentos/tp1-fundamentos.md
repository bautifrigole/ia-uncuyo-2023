# Trabajo Práctico 1: Fundamentos

## Ejercicio 1
### Inteligencia Artificial Débil

La inteligencia artificial débil surge con la siguiente afirmación:

> Cada aspecto del aprendizaje o cualquier otra característica de la inteligencia se puede describir con tanta precisión que se puede hacer una máquina para simularlo.

La inteligencia artificial débil se refiere a la hipótesis de que las máquinas pueden actuar _como si_ fueran inteligentes.

#### Turing Test
Alan Turing rechazó la pregunta "¿Pueden pensar las máquinas?" y lo reemplazó con una prueba de comportamiento. Anticipó muchas objeciones a la posibilidad de máquinas pensantes.

La prueba consiste en que un programa tenga una conversación (a través de mensajes escritos en línea) con un interrogador durante cinco minutos. El interrogador luego tiene que adivinar si la conversación es con un programa o una persona; el programa pasa la prueba si engaña al interrogador el 30% de las veces.

Pocos investigadores de IA prestan atención a la prueba de Turing y prefieren concentrarse en el rendimiento de sus sistemas en tareas prácticas, en lugar de la capacidad de imitar a los humanos.

El propio Turing examinó una amplia variedad de posibles objeciones a la posibilidad de máquinas inteligentes, incluidas prácticamente todas las que se han planteado en el medio siglo transcurrido desde que apareció su artículo. Estas son algunas de ellas.

#### El argumento de la discapacidad
El argumento de la discapacidad afirma que "una máquina nunca puede hacer X". Como ejemplos de X, Turing enumera los siguientes:

Ser amable, ingenioso, hermoso, amistoso, tenga iniciativa, tenga sentido del humor, distinga el bien del mal, enamorarse, aprender de la experiencia, etc. 

Es decir, una máquina nunca podrá tener emociones reales.

#### La objeción matemática

Centrada sobre todo en el Teorema de la Incompletitud de Gödel (1931) que dice que en cualquier sistema lógico suficientemente potente, existen afirmaciones que no pueden ser ni afirmadas ni refutadas dentro de ese sistema a menos que el propio sistema sea inconsistente.

Filósofos han afirmado que este teorema muestra que las máquinas son mentalmente inferiores a los humanos, porque las máquinas son sistemas formales que están limitados por el Teorema de la Incompletitud (no pueden establecer la verdad de su propia oración de Gödel) mientras que los humanos no tienen esa limitación.

#### El argumento desde la informalidad

Turing plantea una de las críticas más influyentes y persistentes a la IA afirmando de que el comportamiento humano es demasiado complejo para ser capturado por un simple conjunto de reglas y que, debido a que las computadoras no pueden hacer más que seguir un conjunto de reglas, no pueden generar un comportamiento tan inteligente como el de los humanos. La incapacidad de capturar todo en un conjunto de reglas lógicas se denomina **problema de calificación** en IA.

### Inteligencia Artificial Fuerte

La inteligencia artificial fuerte se refiere a la hipótesis de que las máquinas que actúan como si fueran inteligentes, _en realidad_ si piensan conscientemente (no sólo _simulan_ pensar).

Muchos filósofos han afirmado que una máquina que pasa la prueba de Turing todavía no estaría pensando en realidad, sino que sería solo una simulación del pensamiento.

Turing responde con el **argumento de la conciencia**, el cual dice que la máquina tiene que ser consciente de sus propios estados mentales y acciones. El punto clave de esto se relaciona con la fenomenología: la máquina tiene que sentir emociones.

Otros se enfocan en la **intencionalidad**, es decir, la cuestión de si las supuestas creencias, deseos y otras representaciones de la máquina son en realidad _sobre_ algo del mundo real.

Turing responde a esto con la pregunta: ¿por qué deberíamos insistir en un estándar más alto para las máquinas que para los humanos? 
Después de todo, en la vida ordinaria nunca tenemos ninguna evidencia directa sobre los estados mentales internos de otros humanos. Sin embargo, Turing dice: “En lugar de discutir continuamente sobre este punto, lo habitual es tener la **convención cortés** de que todo el mundo piensa”.

Turing sugiere que este problema eventualmente desaparecerá por sí solo una vez que las máquinas alcancen un cierto nivel de sofisticación. Esto tendría el efecto de disolver la diferencia entre IA débil y fuerte. 

En contra de esto, se puede insistir en que hay una cuestión de hecho en juego: los humanos tienen mentes reales, y las máquinas pueden tenerlas o no. Para abordar este tema fáctico, necesitamos entender cómo es que los humanos tienen mentes reales, no solo cuerpos que generan procesos neurofisiológicos. Los esfuerzos filosóficos para resolver este problema mente-cuerpo son directamente relevantes para la cuestión de si las máquinas podrían tener mentes reales.

René Descartes (1641) consideró la actividad de pensar de la mente y los procesos físicos del cuerpo, concluyendo que los dos deben existir en reinos separados, lo que ahora se conoce como  **Teoría Dualista**.

La **Teoría Monista** de la mente, a menudo llamada **Fisicalismo**, afirma que la mente no está separada del cuerpo, que los estados mentales son estados físicos. La mayoría de los filósofos modernos de la mente son fisicalistas de una forma u otra, y el fisicalismo permite, al menos en principio, la posibilidad de una IA fuerte. El problema para los fisicalistas es explicar cómo los estados físicos pueden ser simultáneamente **estados mentales**, como sentir dolor, disfrutar de una hamburguesa, etc.

#### Estados mentales y el cerebro en un frasco

La simplicidad de este punto de vista se ve desafiada por algunos experimentos mentales simples.

Imagine que su cerebro fuera removido de su cuerpo al nacer y colocado en un frasco maravillosamente diseñado. El frasco sostiene su cerebro, permitiéndole crecer y desarrollarse. Al mismo tiempo, se envían señales electrónicas a su cerebro desde una simulación por computadora de un mundo completamente ficticio, y las señales motoras de su cerebro se interceptan y se usan para modificar la simulación según corresponda.

La vida simulada que usted vive replica exactamente la vida que habrías vivido si tu cerebro no hubiera sido colocado en el frasco, incluida la comida simulada de hamburguesas simuladas. Por lo tanto, podrías tener un estado cerebral idéntico al de alguien que realmente está comiendo una hamburguesa real, pero sería literalmente falso decir que tienes el estado mental de "saber que uno está comiendo una hamburguesa". No estás comiendo una hamburguesa, nunca has probado una hamburguesa y, por lo tanto, no podrías tener ese estado mental.

Este ejemplo parece contradecir la opinión de que los estados cerebrales determinan los estados mentales. Una forma de resolver el dilema es decir que el contenido de los estados mentales puede interpretarse desde dos puntos de vista diferentes. La visión de **Contenido Amplio** lo interpreta desde el punto de vista de un observador externo omnisciente con acceso a toda la situación, que puede distinguir las diferencias en el mundo. Bajo este punto de vista, el contenido de los estados mentales involucra tanto el estado del cerebro como la historia del entorno. El **Contenido Estrecho**, por otro lado, considera solo el estado del cerebro. El contenido limitado de los estados cerebrales de un consumidor real de hamburguesas y de un “comedor” de “hamburguesas” con cerebro en una cubeta es el mismo en ambos casos.

Esto lleva naturalmente a la idea de que lo que importa sobre un estado cerebral, lo que hace que tenga un tipo de contenido mental y no otro, es su papel **funcional** dentro de la operación mental de la entidad involucrada.

#### Funcionalismo y el experimento de reemplazo cerebral

La **Teoría del Funcionalismo** dice que un estado mental es cualquier condición causal intermedia entre una entrada y una salida. Según la teoría funcionalista, dos sistemas cualesquiera con procesos causales isomórficos tendrían los mismos estados mentales. Por lo tanto, un programa de computadora podría tener los mismos estados mentales que una persona.

Las afirmaciones del funcionalismo se ilustran más claramente con el experimento de reemplazo cerebral. Supongamos que la neurofisiología se ha desarrollado hasta el punto en que el comportamiento de entrada-salida y la conectividad de todas las neuronas del cerebro humano se entienden perfectamente. Supongamos además que podemos construir dispositivos electrónicos microscópicos que imiten este comportamiento y puedan conectarse sin problemas al tejido neural. Por último, supongamos que alguna técnica quirúrgica milagrosa puede reemplazar neuronas individuales con los dispositivos electrónicos correspondientes sin interrumpir el funcionamiento del cerebro como un todo. El experimento consiste en reemplazar gradualmente todas las neuronas de la cabeza de alguien con dispositivos electrónicos.

Por definición del experimento, el comportamiento externo del sujeto debe permanecer invariable en comparación con lo que se observaría si no se realizara la operación. Ahora bien, aunque un tercero no puede determinar fácilmente la presencia o ausencia de conciencia, el sujeto del experimento debería al menos ser capaz de registrar cualquier cambio en su propia experiencia consciente. Aparentemente, hay un choque directo de intuiciones sobre lo que sucedería. Algunos sostienen que su conciencia no se verá afectada y otros sostienen que su conciencia se desvanecería poco a poco por completo.

#### Naturalismo Biológico y la Habitación China

El **Naturalismo Biológico** de John Searle (1980) ha planteado un fuerte desafío al funcionalismo, según el cual los estados mentales son características emergentes de alto nivel causadas por procesos físicos de bajo nivel en las neuronas, y son las propiedades (no especificadas) de las neuronas que importan. Por lo tanto, los estados mentales no se pueden duplicar simplemente sobre la base de algún programa que tenga la misma estructura funcional con el mismo comportamiento de entrada-salida; requeriríamos que el programa se ejecutara en una arquitectura con el mismo poder causal que las neuronas.

Para respaldar su punto de vista, Searle describe un sistema hipotético que claramente está ejecutando un programa y pasa la prueba de Turing, pero que igualmente no entiende nada de sus entradas y salidas. Su conclusión es que ejecutar el programa apropiado (es decir, tener los resultados correctos) no es una condición suficiente para ser una mente.

El sistema consta de un humano, que solo entiende inglés, equipado con un libro de reglas, escrito en inglés, y varias pilas de papel, algunas en blanco, otras con inscripciones indescifrables. El sistema está dentro de una habitación con una pequeña abertura hacia el exterior. Por la abertura aparecen papelitos con símbolos indescifrables. El humano encuentra símbolos coincidentes en el libro de reglas y sigue las instrucciones. Las instrucciones pueden incluir escribir símbolos en tiras de papel nuevas, encontrar símbolos en las pilas, reorganizar las pilas, etc. Eventualmente, las instrucciones harán que uno o más símbolos se transcriban en una hoja de papel que se devuelve al mundo exterior.

Por lo tanto, el ser humano desempeña el papel de la CPU, el libro de reglas es el programa y las pilas de papel son el dispositivo de almacenamiento.

Desde el exterior, vemos un sistema que toma información en forma de oraciones chinas y genera respuestas en chino que son tan "inteligentes" como las de la conversación imaginada por Turing. Searle luego argumenta: la persona en la habitación no entiende chino. El libro de reglas y las pilas de papel, al ser solo pedazos de papel, no entienden chino. Por lo tanto, no hay comprensión del chino. Por lo tanto, según Searle, ejecutar el programa correcto no genera necesariamente comprensión.

#### Conciencia, qualia y la brecha explicativa

La **conciencia** a menudo se divide en aspectos como la comprensión y la autoconciencia. El aspecto en el que nos centraremos es el de la experiencia subjetiva: por qué se siente como algo tener ciertos estados cerebrales (p. ej., mientras se come una hamburguesa), mientras que presumiblemente no se siente como algo tener otros estados físicos (p. ej., ser una roca). El término técnico para la naturaleza intrínseca de las experiencias es **qualia**.

El propio Turing admite que la cuestión de la conciencia es difícil, pero niega que tenga mucha relevancia para la práctica de la IA.

### La ética y los riesgos de desarrollar Inteligencia Artificial

**Riesgos de desarrollar Inteligencia Artificial:**

1. **Las personas podrían perder sus trabajos debido a la automatización.**
La economía industrial moderna se ha vuelto dependiente de las computadoras en general y de ciertos programas de inteligencia artificial en particular. Se podría decir que miles de trabajadores han sido desplazados por estos programas de IA, pero de hecho, si se quitaran los programas de IA, estos puestos de trabajo no existirían, porque la mano de obra humana agregaría un costo inaceptable a las transacciones.

2. **Las personas podrían tener demasiado (o muy poco) tiempo de ocio.**
Se había predecido que las IA iban a realizar tantas tareas humanas que íbamos a tener demasiado tiempo de ocio. Pero en realidad, las personas que trabajan en industrias de conocimiento se han encontrado a sí mismas como parte de un sistema computarizado integrado que opera las 24 horas del día; para mantenerse al día, es necesario trabajar más horas.

3. **Las personas podrían perder su sentido de ser únicas.**
La investigación de la IA puede hacer posible la idea de que los humanos son autómatas, una idea que resulta en una pérdida de autonomía o incluso de humanidad. Pero la humanidad ha sobrevivido a otros reveses a nuestro sentido de singularidad.

4. **Los sistemas de inteligencia artificial podrían ser utilizados para fines indeseables.**
Los poderosos a menudo han utilizado tecnologías avanzadas para reprimir a sus rivales. Por ejemplo, los sistemas autónomos de IA ahora son comunes en el campo de batalla.

También la tecnología de reconocimiento de voz puede conducir a escuchas telefónicas generalizadas y, por lo tanto, a una pérdida de libertades civiles. Algunos aceptan que la informatización conduce a una pérdida de privacidad.

5. **El éxito de la inteligencia artificial podría significar el fin de la raza humana.**
Casi cualquier tecnología tiene el potencial de causar daño en las manos equivocadas, pero con la IA y la robótica, tenemos el nuevo problema de que las manos equivocadas pueden pertenecer a la tecnología misma.

### Mapa mental

![mapa-mental](https://github.com/bautifrigole/ia-uncuyo-2023/assets/64384449/0425359d-8711-4266-bb1c-e06094bb3f03)

### Opinión personal

Opinión personal sobre los enfoques tratados en el capítulo, su alcance, su viabilidad, etc.

Para mitigar los riesgos de la IA, se puede recurrir a la implementación de regulaciones y estándares éticos para el desarrollo y uso de la IA, a la transparencia en el proceso de toma de decisiones de los sistemas de IA y a la inclusión de diversas perspectivas y voces en el desarrollo y uso de la IA. También es importante fomentar la educación y la conciencia pública sobre los riesgos y desafíos éticos asociados con la IA y promover un diálogo abierto y continuo sobre estos temas. Al abordar estos riesgos y desafíos éticos, podemos asegurarnos de que la IA se desarrolle y utilice de manera responsable y ética para el beneficio de la sociedad en general.

## Ejercicio 2

A partir de la lectura del artículo You Are Not a Parrot elaborar un breve comentario defendiendo el uso de la inteligencia artificial generativa a pesar de los comentarios observados en el artículo.
https://nymag.com/intelligencer/article/ai-artificial-intelligence-chatbots-emily-m-bender.html
